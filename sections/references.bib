@misc{computerphileDijkstraAlgorithmComputerphile2017,
  title = {Dijkstra's {{Algorithm}} - {{Computerphile}}},
  author = {{Computerphile}},
  year = {2017},
  month = jan,
  urldate = {2024-05-10},
  abstract = {Dijkstra's Algorithm finds the shortest path between two points. Dr Mike Pound explains how it works. How Sat Nav Works: ~~~{$\bullet~$}Satellite~Navigation~-~Computerphile~~  Slow Loris Attack: ~~~{$\bullet~$}Slow~Loris~Attack~-~Computerphile~~  ~~/~computerphile~~ ~~/~computer\_phile~~ This video was filmed and edited by Sean Riley. Computer Science at the University of Nottingham: http://bit.ly/nottscomputer Computerphile is a sister project to Brady Haran's Numberphile. More at http://www.bradyharan.com}
}

@book{corkeRoboticsVisionControl2023,
  title = {Robotics, {{Vision}} and {{Control}}: {{Fundamental Algorithms}} in {{Python}}},
  shorttitle = {Robotics, {{Vision}} and {{Control}}},
  author = {Corke, Peter},
  year = {2023},
  series = {Springer {{Tracts}} in {{Advanced Robotics}}},
  volume = {146},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-031-06469-2},
  urldate = {2024-04-24},
  copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
  isbn = {978-3-031-06468-5 978-3-031-06469-2},
  langid = {english}
}

@misc{CovarianceMatrixFormula,
  title = {Covariance {{Matrix}} - {{Formula}}, {{Examples}}, {{Definition}}, {{Properties}}},
  journal = {Cuemath},
  urldate = {2024-05-10},
  abstract = {Covariance matrix is a square symmetric matrix that depicts the covariances of a pair of variables and the variance. Understand covariance matrix using solved examples.},
  howpublished = {https://www.cuemath.com/algebra/covariance-matrix/},
  langid = {english},
  file = {/Users/dangkhoachiem/Zotero/storage/VGH4UMUC/covariance-matrix.html}
}

@article{DifferenceQuotient2023,
  title = {Difference Quotient},
  year = {2023},
  month = jan,
  journal = {Wikipedia},
  urldate = {2024-05-12},
  abstract = {In single-variable calculus, the difference quotient is usually the name for the expression                                                               f               (               x               +               h               )               -               f               (               x               )                          h                                     \{{\textbackslash}displaystyle \{{\textbackslash}frac \{f(x+h)-f(x)\}\{h\}\}\}    which when taken to the limit as h approaches 0 gives the derivative of the function f. The name of the expression stems from the fact that it is the quotient of the difference of values of the function by the difference of the corresponding values of its argument (the latter is (x + h) - x = h in this case). The difference quotient is a measure of the average rate of change of the function over an interval (in this case, an interval of length h).:{$\mkern1mu$}237{$\mkern1mu$} The limit of the difference quotient (i.e., the derivative) is thus the instantaneous rate of change. By a slight change in notation (and viewpoint), for an interval [a, b], the difference quotient                                                               f               (               b               )               -               f               (               a               )                                         b               -               a                                                  \{{\textbackslash}displaystyle \{{\textbackslash}frac \{f(b)-f(a)\}\{b-a\}\}\}    is called the mean (or average) value of the derivative of f over the interval [a, b]. This name is justified by the mean value theorem, which states that for a differentiable function f, its derivative f{$\prime$} reaches its mean value at some point in the interval. Geometrically, this difference quotient measures the slope of the secant line passing through the points with coordinates (a, f(a)) and  (b, f(b)). Difference quotients are used as approximations in numerical differentiation, but they have also been subject of criticism in this application. Difference quotients may also find relevance in applications involving Time discretization, where the width of the time step is used for the value of h. The difference quotient is sometimes also called the Newton quotient (after Isaac Newton) or Fermat's difference quotient (after Pierre de Fermat).},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1133565978},
  file = {/Users/dangkhoachiem/Zotero/storage/KG2IVHKT/Difference_quotient.html}
}

@article{Gradient2024,
  title = {Gradient},
  year = {2024},
  month = mar,
  journal = {Wikipedia},
  urldate = {2024-04-30},
  abstract = {In vector calculus, the gradient of a scalar-valued differentiable function                         f                 \{{\textbackslash}displaystyle f\}     of several variables is the vector field (or vector-valued function)                         ∇         f                 \{{\textbackslash}displaystyle {\textbackslash}nabla f\}     whose value at a point                         p                 \{{\textbackslash}displaystyle p\}     gives the direction and the rate of fastest increase. The gradient transforms like a vector under change of basis of the space of variables of                         f                 \{{\textbackslash}displaystyle f\}    . If the gradient of a function is non-zero at a point                         p                 \{{\textbackslash}displaystyle p\}    , the direction of the gradient is the direction in which the function increases most quickly from                         p                 \{{\textbackslash}displaystyle p\}    , and the magnitude of the gradient is the rate of increase in that direction, the greatest absolute directional derivative. Further, a point where the gradient is the zero vector is known as a stationary point. The gradient thus plays a fundamental role in optimization theory, where it is used to minimize a function by gradient descent. In coordinate-free terms, the gradient of a function                         f         (                    r                  )                 \{{\textbackslash}displaystyle f({\textbackslash}mathbf \{r\} )\}     may be defined by: where                         d         f                 \{{\textbackslash}displaystyle df\}     is the total infinitesimal change in                         f                 \{{\textbackslash}displaystyle f\}     for an infinitesimal displacement                          d                    r                          \{{\textbackslash}displaystyle d{\textbackslash}mathbf \{r\} \}    , and is seen to be maximal when                         d                    r                          \{{\textbackslash}displaystyle d{\textbackslash}mathbf \{r\} \}     is in the direction of the gradient                         ∇         f                 \{{\textbackslash}displaystyle {\textbackslash}nabla f\}    . The nabla symbol                         ∇                 \{{\textbackslash}displaystyle {\textbackslash}nabla \}    , written as an upside-down triangle and pronounced "del", denotes the vector differential operator. When a coordinate system is used in which the basis vectors are not functions of position, the gradient is given by the vector whose components are the partial derivatives of                         f                 \{{\textbackslash}displaystyle f\}     at                         p                 \{{\textbackslash}displaystyle p\}    . That is, for                         f         :                                 R                                   n                             {$\rightarrow$}                    R                          \{{\textbackslash}displaystyle f{\textbackslash}colon {\textbackslash}mathbb \{R\} {\textasciicircum}\{n\}{\textbackslash}to {\textbackslash}mathbb \{R\} \}    , its gradient                         ∇         f         :                                 R                                   n                             {$\rightarrow$}                                 R                                   n                                     \{{\textbackslash}displaystyle {\textbackslash}nabla f{\textbackslash}colon {\textbackslash}mathbb \{R\} {\textasciicircum}\{n\}{\textbackslash}to {\textbackslash}mathbb \{R\} {\textasciicircum}\{n\}\}     is defined at the point                         p         =         (                    x                        1                             ,         {\dots}         ,                    x                        n                             )                 \{{\textbackslash}displaystyle p=(x\_\{1\},{\textbackslash}ldots ,x\_\{n\})\}     in n-dimensional space as the vector Note that the above definition for gradient is only defined for the function                         f                 \{{\textbackslash}displaystyle f\}    , if it is differentiable at                         p                 \{{\textbackslash}displaystyle p\}    . There can be functions for which partial derivatives exist in every direction but fail to be differentiable.  For example, the function                         f         (         x         ,         y         )         =                                                                 x                                    2                                               y                                                          x                                    2                                               +                                y                                    2                                                                                  \{{\textbackslash}displaystyle f(x,y)=\{{\textbackslash}frac \{x{\textasciicircum}\{2\}y\}\{x{\textasciicircum}\{2\}+y{\textasciicircum}\{2\}\}\}\}     unless at origin where                         f         (         0         ,         0         )         =         0                 \{{\textbackslash}displaystyle f(0,0)=0\}    , is not differentiable at the origin as it does not have a well defined tangent plane despite having well defined partial derivatives in every direction at the origin. In this particular example, under rotation of x-y coordinate system, the above formula for gradient fails to transform like a vector (gradient becomes dependent on choice of basis for coordinate system) and also fails to point towards the 'steepest ascent' in some orientations. For differentiable functions where the formula for gradient holds, it can be shown to always transform as a vector under transformation of the basis so as to always point towards the fastest increase. The gradient is dual to the total derivative                         d         f                 \{{\textbackslash}displaystyle df\}    : the value of the gradient at a point is a tangent vector -- a vector at each point; while the value of the derivative at a point is a cotangent vector -- a linear functional on vectors. They are related in that the dot product of the gradient of                         f                 \{{\textbackslash}displaystyle f\}     at a point                         p                 \{{\textbackslash}displaystyle p\}     with another tangent vector                                    v                          \{{\textbackslash}displaystyle {\textbackslash}mathbf \{v\} \}     equals the directional derivative of                         f                 \{{\textbackslash}displaystyle f\}     at                         p                 \{{\textbackslash}displaystyle p\}     of the function along                                    v                          \{{\textbackslash}displaystyle {\textbackslash}mathbf \{v\} \}    ; that is,                         ∇         f         (         p         )         {$\cdot$}                    v                  =                                                {$\partial$}               f                                         {$\partial$}                                v                                                         (         p         )         =         d                    f                        p                             (                    v                  )                 \{{\textbackslash}textstyle {\textbackslash}nabla f(p){\textbackslash}cdot {\textbackslash}mathbf \{v\} =\{{\textbackslash}frac \{{\textbackslash}partial f\}\{{\textbackslash}partial {\textbackslash}mathbf \{v\} \}\}(p)=df\_\{p\}({\textbackslash}mathbf \{v\} )\}    .  The gradient admits multiple generalizations to more general functions on manifolds; see {\S} Generalizations.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1216115382},
  file = {/Users/dangkhoachiem/Zotero/storage/LNE3B7SP/Gradient.html}
}

@article{GradientDescent2024,
  title = {Gradient Descent},
  year = {2024},
  month = mar,
  journal = {Wikipedia},
  urldate = {2024-05-01},
  abstract = {Gradient descent is a method for unconstrained mathematical optimization. It is a first-order iterative algorithm for finding a local minimum of a differentiable multivariate function. The idea is to take repeated steps in the opposite direction of the gradient (or approximate gradient) of the function at the current point, because this is the direction of steepest descent. Conversely, stepping in the direction of the gradient will lead to a local maximum of that function; the procedure is then known as gradient ascent. It is particularly useful in machine learning for minimizing the cost or loss function. Gradient descent should not be confused with local search algorithms, although both are iterative methods for optimization. Gradient descent is generally attributed to Augustin-Louis Cauchy, who first suggested it in 1847. Jacques Hadamard independently proposed a similar method in 1907. Its convergence properties for non-linear optimization problems were first studied by Haskell Curry in 1944, with the method becoming increasingly well-studied and used in the following decades. A simple extension of gradient descent, stochastic gradient descent, serves as the most basic algorithm used for training most deep networks today.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1215767849},
  file = {/Users/dangkhoachiem/Zotero/storage/IK26BHNS/Gradient_descent.html}
}

@misc{GradientVectorMultivariable,
  title = {The Gradient Vector {\textbar} {{Multivariable}} Calculus (Article) {\textbar} {{Khan Academy}}},
  urldate = {2024-04-30},
  howpublished = {https://www.khanacademy.org/\_render},
  langid = {english},
  file = {/Users/dangkhoachiem/Zotero/storage/WAMGLWMR/the-gradient.html}
}

@inproceedings{hessRealtimeLoopClosure2016,
  title = {Real-Time Loop Closure in {{2D LIDAR SLAM}}},
  booktitle = {2016 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Hess, Wolfgang and Kohler, Damon and Rapp, Holger and Andor, Daniel},
  year = {2016},
  month = may,
  pages = {1271--1278},
  publisher = {IEEE},
  address = {Stockholm, Sweden},
  doi = {10.1109/ICRA.2016.7487258},
  urldate = {2024-04-24},
  abstract = {Portable laser range-finders, further referred to as LIDAR, and simultaneous localization and mapping (SLAM) are an efficient method of acquiring as-built floor plans. Generating and visualizing floor plans in real-time helps the operator assess the quality and coverage of capture data. Building a portable capture platform necessitates operating under limited computational resources. We present the approach used in our backpack mapping platform which achieves real-time mapping and loop closure at a 5 cm resolution. To achieve realtime loop closure, we use a branch-and-bound approach for computing scan-to-submap matches as constraints. We provide experimental results and comparisons to other well known approaches which show that, in terms of quality, our approach is competitive with established techniques.},
  isbn = {978-1-4673-8026-3},
  langid = {english},
  file = {/Users/dangkhoachiem/Zotero/storage/TVU6TLMW/Hess et al. - 2016 - Real-time loop closure in 2D LIDAR SLAM.pdf}
}

@misc{InstallationROSDocumentation,
  title = {Installation --- {{ROS}} 2 {{Documentation}}: {{Humble}} Documentation},
  urldate = {2024-05-12},
  howpublished = {https://docs.ros.org/en/humble/Installation.html},
  file = {/Users/dangkhoachiem/Zotero/storage/9V97Y6QB/Installation.html}
}

@article{macenskiDesksROSMaintainers2023,
  title = {From the {{Desks}} of {{ROS Maintainers}}: {{A Survey}} of {{Modern}} \& {{Capable Mobile Robotics Algorithms}} in the {{Robot Operating System}} 2},
  shorttitle = {From the {{Desks}} of {{ROS Maintainers}}},
  author = {Macenski, Steve and Moore, Tom and Lu, David and Merzlyakov, Alexey and Ferguson, Michael},
  year = {2023},
  month = oct,
  journal = {Robotics and Autonomous Systems},
  volume = {168},
  eprint = {2307.15236},
  primaryclass = {cs},
  pages = {104493},
  issn = {09218890},
  doi = {10.1016/j.robot.2023.104493},
  urldate = {2024-05-05},
  abstract = {The Robot Operating System 2 (ROS 2) is rapidly impacting the intelligent machines sector - on space missions, large agriculture equipment, multi-robot fleets, and more. Its success derives from its focused design and improved capabilities targeting product-grade and modern robotic systems. Following ROS 2's example, the mobile robotics ecosystem has been fully redesigned based on the transformed needs of modern robots and is experiencing active development not seen since its inception.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Robotics},
  file = {/Users/dangkhoachiem/Zotero/storage/Q73ZXNSW/Macenski et al. - 2023 - From the Desks of ROS Maintainers A Survey of Mod.pdf}
}

@article{MarkovProperty2024a,
  title = {Markov Property},
  year = {2024},
  month = apr,
  journal = {Wikipedia},
  urldate = {2024-04-26},
  abstract = {In probability theory and statistics, the term Markov property refers to the memoryless property of a stochastic process, which means that its future evolution is independent of its history. It is named after the Russian mathematician  Andrey Markov. The term strong Markov property is similar to the Markov property, except that the meaning of "present" is defined in terms of a random variable known as a stopping time. The term Markov assumption is used to describe a model where the Markov property is assumed to hold, such as a hidden Markov model. A Markov random field extends this property to two or more dimensions or to random variables defined for an interconnected network of items. An example of a model for such a field is the Ising model. A discrete-time stochastic process satisfying the Markov property is known as a Markov chain.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1217007815},
  file = {/Users/dangkhoachiem/Zotero/storage/FG2WCJA4/Markov_property.html}
}

@misc{matlabUnderstandingParticleFilter2020,
  title = {Understanding the {{Particle Filter}} {\textbar}  {\textbar} {{Autonomous Navigation}}, {{Part}} 2},
  author = {{MATLAB}},
  year = {2020},
  month = jul,
  urldate = {2024-04-26},
  abstract = {Watch the first video in this series here: ~~~{$\bullet~$}What~Is~Autonomous~Navigation?~{\textbar}~Auto...~~ This video presents a high-level understanding of the particle filter and shows how it can be used in Monte Carlo localization to determine the pose of a mobile robot inside a building.   We'll cover why the particle filter is better suited for this type of problem than the traditional Kalman filter because of its ability to handle non-Gaussian probability distributions. Additional Resources: - More details on dead reckoning, MATLAB Tech Talk video: https://bit.ly/37T9BRT - Understanding the Kalman Filter, MATLAB Tech Talk Series: https://bit.ly/314rLia - Another good description of the particle filter: ~~~{$\bullet~$}Particle~Filter~Explained~without~Equ...~~ - Download ebook: Sensor Fusion and Tracking for Autonomous Systems: An Overview - https://bit.ly/2YZxvXA - Download white paper: Sensor Fusion and Tracking for Autonomous Systems - https://bit.ly/2YZxvXA - A Tutorial on Particle Filtering and Smoothing (includes AMCL). Paper by Doucet and Johansen: https://www.stats.ox.ac.uk/{\textasciitilde}doucet/do... -------------------------------------------------------------------------------------------------------- Get a free product trial: https://goo.gl/ZHFb5u Learn more about MATLAB: https://goo.gl/8QV7ZZ Learn more about Simulink: https://goo.gl/nqnbLe See what's new in MATLAB and Simulink: https://goo.gl/pgGtod {\copyright} 2020 The MathWorks, Inc. MATLAB and Simulink are registered trademarks of The MathWorks, Inc.  See www.mathworks.com/trademarks for a list of additional trademarks. Other product or brand names may be trademarks or registered trademarks of their respective holders.}
}

@misc{Nav2Nav2Documentation,
  title = {Nav2 --- {{Nav2}} 1.0.0 Documentation},
  urldate = {2024-04-30},
  howpublished = {https://navigation.ros.org/},
  file = {/Users/dangkhoachiem/Zotero/storage/VAWWULAE/navigation.ros.org.html}
}

@misc{NavFnPlannerNav2,
  title = {{{NavFn Planner}} --- {{Nav2}} 1.0.0 Documentation},
  urldate = {2024-04-30},
  howpublished = {https://navigation.ros.org/configuration/packages/configuring-navfn.html},
  file = {/Users/dangkhoachiem/Zotero/storage/3MT9QBRB/configuring-navfn.html}
}

@inproceedings{philippsenInterpolatedDynamicNavigation2005,
  title = {An {{Interpolated Dynamic Navigation Function}}},
  booktitle = {Proceedings of the 2005 {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  author = {Philippsen, R. and Siegwart, R.},
  year = {2005},
  pages = {3782--3789},
  publisher = {IEEE},
  address = {Barcelona, Spain},
  doi = {10.1109/ROBOT.2005.1570697},
  urldate = {2024-04-30},
  abstract = {The E{$\ast$} algorithm is a path planning method capable of dynamic replanning and user-configurable path cost interpolation. It calculates a navigation function as a sampling of an underlying smooth goal distance that takes into account a continuous notion of risk that can be controlled in a fine-grained manner. E{$\ast$} results in more appropriate paths during gradient descent. Dynamic replanning means that changes in the environment model can be repaired to avoid the expenses of complete replanning. This helps compensating for the increased computational effort required for interpolation. We present the theoretical basis and a working implementation, as well as measurements of the algorithm's precision, topological correctness, and computational effort.},
  isbn = {978-0-7803-8914-4},
  langid = {english},
  file = {/Users/dangkhoachiem/Zotero/storage/MQMGHBJV/eth-8123-01.pdf}
}

@misc{ROBOTISEManual,
  title = {{{ROBOTIS}} E-{{Manual}}},
  journal = {ROBOTIS e-Manual},
  urldate = {2024-04-24},
  abstract = {e-Manual wiki},
  howpublished = {https://emanual.robotis.com/docs/en/platform/turtlebot3/features/},
  langid = {english},
  file = {/Users/dangkhoachiem/Zotero/storage/KZK9PRSW/features.html}
}

@misc{robotmaniaDynamicWindowApproach2020,
  title = {Dynamic {{Window Approach Tutorial}}},
  author = {{robot mania}},
  year = {2020},
  month = oct,
  urldate = {2024-05-12},
  abstract = {The program is here https://drive.google.com/drive/folder...}
}

@article{RotationMatrix2024,
  title = {Rotation Matrix},
  year = {2024},
  month = apr,
  journal = {Wikipedia},
  urldate = {2024-04-26},
  abstract = {In linear algebra, a rotation matrix is a transformation matrix that is used to perform a rotation in Euclidean space. For example, using the convention below, the matrix                        R         =                                 [                                                                cos                   ⁡                   {\texttheta}                                                     -                   sin                   ⁡                   {\texttheta}                                                                                   sin                   ⁡                   {\texttheta}                                                     cos                   ⁡                   {\texttheta}                                                          ]                                     \{{\textbackslash}displaystyle R=\{{\textbackslash}begin\{bmatrix\}{\textbackslash}cos {\textbackslash}theta \&-{\textbackslash}sin {\textbackslash}theta {\textbackslash}{\textbackslash}{\textbackslash}sin {\textbackslash}theta \&{\textbackslash}cos {\textbackslash}theta {\textbackslash}end\{bmatrix\}\}\}    rotates points in the xy plane counterclockwise through an angle {\texttheta} about the origin of a two-dimensional Cartesian coordinate system. To perform the rotation on a plane point with standard coordinates v = (x, y), it should be written as a column vector, and multiplied by the matrix R:                        R                    v                  =                                 [                                                                cos                   ⁡                   {\texttheta}                                                     -                   sin                   ⁡                   {\texttheta}                                                                                   sin                   ⁡                   {\texttheta}                                                     cos                   ⁡                   {\texttheta}                                                          ]                                                     [                                                                x                                                                                   y                                                          ]                             =                                 [                                                                x                   cos                   ⁡                   {\texttheta}                   -                   y                   sin                   ⁡                   {\texttheta}                                                                                   x                   sin                   ⁡                   {\texttheta}                   +                   y                   cos                   ⁡                   {\texttheta}                                                          ]                             .                 \{{\textbackslash}displaystyle R{\textbackslash}mathbf \{v\} =\{{\textbackslash}begin\{bmatrix\}{\textbackslash}cos {\textbackslash}theta \&-{\textbackslash}sin {\textbackslash}theta {\textbackslash}{\textbackslash}{\textbackslash}sin {\textbackslash}theta \&{\textbackslash}cos {\textbackslash}theta {\textbackslash}end\{bmatrix\}\}\{{\textbackslash}begin\{bmatrix\}x{\textbackslash}{\textbackslash}y{\textbackslash}end\{bmatrix\}\}=\{{\textbackslash}begin\{bmatrix\}x{\textbackslash}cos {\textbackslash}theta -y{\textbackslash}sin {\textbackslash}theta {\textbackslash}{\textbackslash}x{\textbackslash}sin {\textbackslash}theta +y{\textbackslash}cos {\textbackslash}theta {\textbackslash}end\{bmatrix\}\}.\}    If x and y are the endpoint coordinates of a vector, where x is cosine and y is sine, then the above equations become the trigonometric summation angle formulae. Indeed, a rotation matrix can be seen as the trigonometric summation angle formulae in matrix form. One way to understand this is to say we have a vector at an angle 30{$^\circ$} from the x axis, and we wish to rotate that angle by a further 45{$^\circ$}. We simply need to compute the vector endpoint coordinates at 75{$^\circ$}. The examples in this article apply to active rotations of vectors counterclockwise in a right-handed coordinate system (y counterclockwise from x) by pre-multiplication (R on the left).  If any one of these is changed (such as rotating axes instead of vectors, a passive transformation), then the inverse of the example matrix should be used, which coincides with its transpose. Since matrix multiplication has no effect on the zero vector (the coordinates of the origin), rotation matrices describe rotations about the origin. Rotation matrices provide an algebraic description of such rotations, and are used extensively for computations in geometry, physics, and computer graphics. In some literature, the term rotation is generalized to include improper rotations, characterized by orthogonal matrices with a determinant of -1 (instead of +1). These combine proper rotations with reflections (which invert orientation). In other cases, where reflections are not being considered, the label proper may be dropped. The latter convention is followed in this article. Rotation matrices are square matrices, with real entries. More specifically, they can be characterized as orthogonal matrices with determinant 1; that is, a square matrix R is a rotation matrix if and only if RT = R-1 and det R = 1. The set of all orthogonal matrices of size  n with determinant +1 is a representation of a group known as the special orthogonal group SO(n), one example of which is the rotation group SO(3). The set of all orthogonal matrices of size n with determinant +1 or -1 is a representation of the (general) orthogonal group O(n).},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1218915039},
  file = {/Users/dangkhoachiem/Zotero/storage/8MVA3ALP/Rotation_matrix.html}
}

@misc{SolvingNonlinearLeast,
  title = {Solving {{Non-linear Least Squares}} --- {{Ceres Solver}}},
  urldate = {2024-05-03},
  howpublished = {http://ceres-solver.org/nnls\_solving.html},
  file = {/Users/dangkhoachiem/Zotero/storage/RJQXLM58/nnls_solving.html}
}

@book{thrunProbabilisticRobotics2006,
  title = {Probabilistic Robotics},
  author = {Thrun, Sebastian and Burgard, Wolfram and Fox, Dieter},
  year = {2006},
  series = {Intelligent Robotics and Autonomous Agents},
  edition = {Nachdruck},
  publisher = {MIT Press},
  address = {Cambridge, Mass. London},
  isbn = {978-0-262-20162-9},
  langid = {english}
}
